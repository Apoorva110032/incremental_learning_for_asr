run_num: 1
data_dir: ../data
result_dir: ../results/tokenizer

text_file: ../results/text.txt
skip_data_preparation: false

vocab_size: 256
model_type: bpe  # bpe, unigram, char or word
character_coverage: 1.0